# ===============================
# ðŸ“¦ IMPORTS
# ===============================
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch


# ===============================
# ðŸ”¹ LOAD MODELS
# ===============================

print("ðŸ”„ Loading models...")

# Embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Generator model
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")
llm = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-large")
print("âœ… Models loaded.")


# ===============================
# ðŸ”¹ FILE READING
# ===============================
def file_open():
    with open("data/os.txt", "r", encoding="utf-8") as f:
        return f.read()


# ===============================
# ðŸ”¹ CHUNKING
# ===============================
def chunk_text(text, chunk_size=500):
    chunks = []
    current = ""

    for line in text.split("\n"):
        line = line.strip()
        if not line:
            continue

        if len(current) + len(line) <= chunk_size:
            current += line + " "
        else:
            chunks.append(current.strip())
            current = line + " "

    if current:
        chunks.append(current.strip())

    return chunks


# ===============================
# ðŸ”¹ CREATE EMBEDDINGS
# ===============================
def create_embeddings(chunks):
    vectors = embedding_model.encode(chunks)
    return np.array(vectors).astype("float32")


# ===============================
# ðŸ”¹ BUILD FAISS INDEX
# ===============================
def build_faiss_index(vectors):
    dimension = vectors.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(vectors)
    return index


# ===============================
# ðŸ”¹ SEARCH FUNCTION
# ===============================
def search(query, index, chunks, k=3):
    query_vector = embedding_model.encode([query])
    query_vector = np.array(query_vector).astype("float32")

    distances, indices = index.search(query_vector, k)

    results = []
    for i in indices[0]:
        results.append(chunks[i])

    return results


# ===============================
# ðŸ”¹ GENERATE FINAL ANSWER (Improved Prompt)
# ===============================
def generate_answer(query, retrieved_chunks):
    context = "\n".join(retrieved_chunks)

    prompt = f"""
You are an educational assistant.

Using ONLY the context provided below, write a detailed, clear, and well-structured answer.
If there are multiple points, list them clearly with explanations.
Do not add information that is not in the context.

Context:
{context}

Question:
{query}

Detailed Answer:
"""

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=1024
    )

    outputs = llm.generate(
        **inputs,
        max_length=400,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer


# ===============================
# ðŸš€ MAIN PIPELINE
# ===============================
def main():
    print("ðŸ“– Loading document...")
    text = file_open()

    print("âœ‚ï¸ Chunking text...")
    chunks = chunk_text(text)
    print(f"ðŸ“¦ Total Chunks Created: {len(chunks)}")

    print("ðŸ§  Creating embeddings...")
    vectors = create_embeddings(chunks)

    print("ðŸ” Building FAISS index...")
    index = build_faiss_index(vectors)

    print("\nâœ… RAG system ready!\n")

    while True:
        query = input("Ask a question (type 'exit' to quit): ")

        if query.lower() == "exit":
            print("ðŸ‘‹ Exiting RAG system.")
            break

        retrieved = search(query, index, chunks, k=3)

        answer = generate_answer(query, retrieved)

        print("\nðŸ¤– Generated Answer:\n")
        print(answer)
        print("\n" + "="*60 + "\n")


if __name__ == "__main__":
    main()
